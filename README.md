# Datasus - Pipeline de Dados com Databricks, Snowflake e dbt
## ðŸ“Œ Contexto
Este projeto foi desenvolvido como parte do Desafio Final de Engenharia de Dados da triggo.ai, com o objetivo de projetar e implementar uma soluÃ§Ã£o de engenharia de dados para a Health Insights Brasil, uma startup fictÃ­cia dedicada a tornar os dados de saÃºde pÃºblica do DataSUS mais acessÃ­veis e prontos para anÃ¡lise.

A soluÃ§Ã£o proposta simula um pipeline de dados completo, desde a ingestÃ£o dos dados brutos atÃ© a modelagem dimensional no formato Star Schema, utilizando **Databricks, Snowflake e dbt**.


## ðŸ›  Tecnologias Utilizadas
**Databricks** â€“ IngestÃ£o e processamento inicial dos dados (camadas raw e clean).

**Snowflake** â€“ Armazenamento e modelagem dimensional (camada gold).

**dbt** â€“ TransformaÃ§Ãµes, documentaÃ§Ã£o, testes e organizaÃ§Ã£o da modelagem.

**Python / PySpark** â€“ Scripts de ingestÃ£o e prÃ©-processamento.

**Delta Lake**â€“ OtimizaÃ§Ã£o de armazenamento e versionamento no Databricks.

## ðŸ“‚ Estrutura do Projeto

```bash
bash

'''
.
â”œâ”€â”€ extracao-dados.ipynb              # Notebook que simula extraÃ§Ã£o dos dados DataSUS (camada raw)
â”œâ”€â”€ execucao-projeto-snowflake.ipynb  # Notebook que executa o dbt apontando para o Snowflake
â”œâ”€â”€ dados_datasus.csv                 # Arquivo de dados exemplo (simulaÃ§Ã£o API DataSUS)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ databricks/                   # Modelos dbt da camada clean
â”‚   â””â”€â”€ snowflake/                    # Modelos dbt da camada gold (dimensional)
â”œâ”€â”€ dados_simulacao/
â”‚   â””â”€â”€ dados_datasus.csv             # Arquivo usado como fonte de dados para o projeto
â”‚   â””â”€â”€ simulando-dados.txt           # Arquivo usado para simular a etapa clean no Snowflake
â”œâ”€â”€ profiles.yml                      # Arquivo de configuraÃ§Ã£o dbt (nÃ£o incluÃ­do por seguranÃ§a)
â””â”€â”€ README.md                         # Este documento

```

## ðŸ“‹ PrÃ©-requisitos
Antes de executar o projeto, vocÃª precisa ter:

**1. Conta no Databricks** (workspace ativo).

**2. Conta no Snowflake** com:
    - Warehouse criado (COMPUTE_WH2)

    - Database criado (DW_SAUDE)

    - Schemas:

        - fontes (para dados brutos)

        - gold (para modelo dimensional)

**3. dbt-core** e adaptadores instalados:
```bash
python
pip install dbt-core dbt-snowflake dbt-databricks

```
**4. profiles.yml** configurado para Databricks e Snowflake. Foi incluido nesse repositÃ³rio um documento de exemplo de como o arquivo deve estar configurado.

**5. Upload** do arquivo dados_datasus.csv no schema fontes do Databricks. O arquivo estÃ¡ localizado na pasta dados_simulacao.

**6. Simular** dados clean no Snowflake. Fois colocado um arquivo para essa simulaÃ§Ã£o na pasta dados_simulacao nomeado como simulando-dados.txt

OBS: a etapa 6 Ã© necessÃ¡ria pois os dados nÃ£o sÃ£o salvos em um provedor de nuvem que permita que o Snowflake tenha acesso a esses dados.

## ðŸš€ Como Executar
Para executar temos duas opÃ§Ãµes. Executar cada um dos notebooks ou criar um job para orquestrar a execuÃ§Ã£o.

### 1. ExecuÃ§Ã£o dos notebooks
- No Databricks, abra e execute o notebook extracao-dados.ipynb.
- Abra o notebook execucao-projeto-snowflake.ipynb

### 2. OrquestraÃ§Ã£o
No Databricks vÃ¡ em **Jobs & Pipelines** e crie um novo job. Esse job deverÃ¡ ter 2 etapas sendo cada etapa para a execuÃ§Ã£o de um notebook. Preencha as informaÃ§Ãµes da seguinte forma:
- Task Name: crie um nome para a etapa a ser executada
- Type: selecione Notebook
- Source: Workspace
- Path: qual o caminho onde estÃ¡ o repositÃ³rio do projeto

## ðŸ“Š Modelo Dimensional
O modelo segue um **Star Schema** com:
- DimensÃµes: dim_calendario, dim_hospital, dim_informacoes_hospitalares, dim_paciente
- Fato: fato_sih

Essas tabelas permitem anÃ¡lises de indicadores de saÃºde pÃºblica, como:
- DistribuiÃ§Ã£o de doenÃ§as por regiÃ£o.
- TendÃªncias temporais de internaÃ§Ãµes.
- Procedimentos mais realizados por faixa etÃ¡ria e sexo.

DocumentaÃ§Ã£o
DocumentaÃ§Ã£o disponÃ­vel via dbt docs generate e dbt docs serve.

## Continuidade para o projeto
- Conectar a pipeline a **uma fonte de dados externa** e armazenar os arquivos brutos em um provedor de nuvem, como **AWS S3**, para garantir escalabilidade e persistÃªncia.
- Implementar **carga incremental** para otimizar o processamento e reduzir custos de execuÃ§Ã£o.
- **Adicionar novas fontes de dados**, ampliando o escopo de anÃ¡lise e permitindo cruzamentos com diferentes conjuntos do DataSUS ou outras bases pÃºblicas.

## ðŸ“œ Desenvolvimento
Projeto desenvolvido para fins educacionais no Desafio Final de Engenharia de Dados da triggo.ai.