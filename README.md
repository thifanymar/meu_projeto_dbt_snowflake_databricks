# Datasus - Pipeline de Dados com Databricks, Snowflake e dbt
## üìå Contexto
Este projeto foi desenvolvido como parte do Desafio Final de Engenharia de Dados da triggo.ai, com o objetivo de projetar e implementar uma solu√ß√£o de engenharia de dados para a Health Insights Brasil, uma startup fict√≠cia dedicada a tornar os dados de sa√∫de p√∫blica do DataSUS mais acess√≠veis e prontos para an√°lise.

A solu√ß√£o proposta simula um pipeline de dados completo, desde a ingest√£o dos dados brutos at√© a modelagem dimensional no formato Star Schema, utilizando **Databricks, Snowflake e dbt**.


## üõ† Tecnologias Utilizadas
**Databricks** ‚Äì Ingest√£o e processamento inicial dos dados (camadas raw e clean).

**Snowflake** ‚Äì Armazenamento e modelagem dimensional (camada gold).

**dbt** ‚Äì Transforma√ß√µes, documenta√ß√£o, testes e organiza√ß√£o da modelagem.

**Python / PySpark** ‚Äì Scripts de ingest√£o e pr√©-processamento.

**Delta Lake**‚Äì Otimiza√ß√£o de armazenamento e versionamento no Databricks.

## üìÇ Estrutura do Projeto

```bash
bash

'''
.
‚îú‚îÄ‚îÄ extracao-dados.ipynb              # Notebook que simula extra√ß√£o dos dados DataSUS (camada raw)
‚îú‚îÄ‚îÄ execucao-projeto-snowflake.ipynb  # Notebook que executa o dbt apontando para o Snowflake
‚îú‚îÄ‚îÄ dados_datasus.csv                 # Arquivo de dados exemplo (simula√ß√£o API DataSUS)
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ databricks/                   # Modelos dbt da camada clean
‚îÇ   ‚îî‚îÄ‚îÄ snowflake/                    # Modelos dbt da camada gold (dimensional)
‚îú‚îÄ‚îÄ dados_simulacao/
‚îÇ   ‚îî‚îÄ‚îÄ dados_datasus.csv             # Arquivo usado como fonte de dados para o projeto
‚îÇ   ‚îî‚îÄ‚îÄ simulando-dados.txt           # Arquivo usado para simular a etapa clean no Snowflake
‚îú‚îÄ‚îÄ profiles.yml                      # Arquivo de configura√ß√£o dbt (n√£o inclu√≠do por seguran√ßa)
‚îî‚îÄ‚îÄ README.md                         # Este documento

```

## üìã Pr√©-requisitos
Antes de executar o projeto, voc√™ precisa ter:

**1. Conta no Databricks** (workspace ativo).

**2. Conta no Snowflake** com:
    - Warehouse criado (COMPUTE_WH2)

    - Database criado (DW_SAUDE)

    - Schemas:

        - fontes (para dados brutos)

        - gold (para modelo dimensional)

**3. dbt-core** e adaptadores instalados:
```bash
python
pip install dbt-core dbt-snowflake dbt-databricks

```
**4. profiles.yml** configurado para Databricks e Snowflake. Foi incluido nesse reposit√≥rio um documento de exemplo de como o arquivo deve estar configurado.

**5. Upload** do arquivo dados_datasus.csv no schema fontes do Databricks. O arquivo est√° localizado na pasta dados_simulacao.

**6. Simular** dados clean no Snowflake. Fois colocado um arquivo para essa simula√ß√£o na pasta dados_simulacao nomeado como simulando-dados.txt

> Observa√ß√£o:
> A etapa 6 √© necess√°ria porque os dados n√£o est√£o armazenados em um provedor de nuvem compat√≠vel com  acesso direto pelo Snowflake.
> Na pasta dados_simulacao est√° dispon√≠vel o arquivo usado para extrair os dados do DATASUS, gerar o arquivo CSV e realizar o download.

## üöÄ Como Executar
Para executar temos duas op√ß√µes. Executar cada um dos notebooks ou criar um job para orquestrar a execu√ß√£o.

### 1. Execu√ß√£o dos notebooks
- No Databricks, abra e execute o notebook extracao-dados.ipynb.
- Abra o notebook execucao-projeto-snowflake.ipynb

### 2. Orquestra√ß√£o
No Databricks v√° em **Jobs & Pipelines** e crie um novo job. Esse job dever√° ter 2 etapas sendo cada etapa para a execu√ß√£o de um notebook. Preencha as informa√ß√µes da seguinte forma:
- Task Name: crie um nome para a etapa a ser executada
- Type: selecione Notebook
- Source: Workspace
- Path: qual o caminho onde est√° o reposit√≥rio do projeto

## üìä Modelo Dimensional
O modelo segue um **Star Schema** com:
- Dimens√µes: dim_calendario, dim_hospital, dim_informacoes_hospitalares, dim_paciente
- Fato: fato_sih

Essas tabelas permitem an√°lises de indicadores de sa√∫de p√∫blica, como:
- Distribui√ß√£o de doen√ßas por regi√£o.
- Tend√™ncias temporais de interna√ß√µes.
- Procedimentos mais realizados por faixa et√°ria e sexo.

Documenta√ß√£o
Documenta√ß√£o dispon√≠vel via dbt docs generate e dbt docs serve.

## Continuidade para o projeto
- Conectar a pipeline a **uma fonte de dados externa** e armazenar os arquivos brutos em um provedor de nuvem, como **AWS S3**, para garantir escalabilidade e persist√™ncia.
- Implementar **carga incremental** para otimizar o processamento e reduzir custos de execu√ß√£o.
- **Adicionar novas fontes de dados**, ampliando o escopo de an√°lise e permitindo cruzamentos com diferentes conjuntos do DataSUS ou outras bases p√∫blicas.

## Dashboard desenvolvido
Link:
https://app.powerbi.com/view?r=eyJrIjoiYTEyYmRmODAtZmJjNi00MzM1LTlmMWMtNjg5YWVmYzg1Yjk1IiwidCI6IjQ5Njk0NmExLWE2YzktNDQxOS1iZWZlLTk4OTBkMzgwNjdkNCJ9

## üìú Desenvolvimento
Projeto desenvolvido para fins educacionais no Desafio Final do Bootcamp de Engenharia de Dados da triggo.ai.